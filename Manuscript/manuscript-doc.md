_Please install [MathJax Plugin for Github](https://chrome.google.com/webstore/detail/mathjax-plugin-for-github/ioemnmodlmafdkllaclgeombjnmnbima/related) or [Math Anywhere](https://chrome.google.com/webstore/detail/math-anywhere/gebhifiddmaaeecbaiemfpejghjdjmhc) Chrome plug-ins to view equations_  

## Braitenberg Vehicles as Developmental Neurosimulation
Bradly Alicea<sup>1,2</sup>, Stefan Dvoretskii<sup>3</sup>, Ziyi Gong<sup>4</sup>, Ankit Gupta<sup>5</sup>, and Jesse Parent<sup>6</sup>

<sup>1</sup> Orthogonal Research and Education Laboratory, <sup>2</sup> OpenWorm Foundation, <sup>3</sup> Technische Universität München, <sup>4</sup> University of Pittsburgh, <sup>5</sup> IIT Kharagpur, <sup>6</sup> SUNY Albany.

### Abstract
The connection between brain and behavior is a longstanding issue in the areas of  behavioral science, artificial intelligence, and neurobiology. Particularly in artificial intelligence research, behavior is generated by a black box approximating the brain. As is standard among models of artificial and biological neural networks, an analogue of the fully mature brain is presented as a blank slate. This model generates outputs and behaviors from a priori associations, yet this does not consider the realities of biological development and developmental learning. Our purpose is to model the development of an artificial organism that exhibits complex behaviors. We will introduce our approach, which is to use Braitenberg Vehicles (BVs) to model the development of an artificial nervous system. The resulting developmental BVs will generate behaviors that range from stimulus responses to group behavior that resembles collective motion. Next, we will situate this work in the domain of artificial brain networks. Then we will focus on broader themes such as embodied cognition, feedback, and emergence. Our perspective will then be exemplified by three software instantiations that demonstrate how a BV-genetic algorithm hybrid model, multisensory Hebbian learning model, and multi-agent approaches can be used to approach BV development. We introduce use cases such as optimized spatial cognition (vehicle-genetic algorithm hybrid model), hinges connecting behavioral and neural models (multisensory Hebbian learning model), and cumulative classification (multi-agent approaches). In conclusion, we will revisit concepts related to our approach and how they might guide future development.  

### Introduction
How do we understand the emergence of a connected nervous system, particularly in terms of how it leads to neural function and behavior? One way is to infer the co-occurrence of neural cell differentiation in a model organism [1,2]. This requires a small connectome in which cell differentiation can be tracked. Even for organisms such as the nematode _Caenorhabditis elegans_ [3], direct experimentation is difficult. An embodied _in silico_ system with a generalized nervous system would provide a means to both modify the developmental process and directly observe all possible developmental outcomes. Utilizing an abstraction to study hard-to-observe questions is in fact consistent with how theoretical modeling and simulations have been used throughout the history of neuroscience [4]. We propose that Braitenberg Vehicles (BV) [5] can be used as a means to construct such simulations. Originally proposed by Valentino Braitenberg, BVs are an embodied model of a simple nervous system. The minimalist architecture allows us to focus on the connection between an embodied  connectome and its behavioral outputs. It is of note that Braitenberg’s original vehicles were designed as thought experiments to show how seemingly complex behaviors may emerge from hard-wired nervous systems and phenotypes. Our approach differs in that we allow nervous systems to develop using a variety of techniques. We will introduce a general computational model, followed by specific instantiations involving different aspects of cognition.  

#### Motivation
This work is motivated by a desire to understand neurodevelopment balanced with a need to establish an in silico model system that allows us to simulate processes such as learning, plasticity, and the regulation of behavior. Of particular interest is a model which allows us to model global structures such as the connectome [6]. An artificial connectome that develops in the context of a controlled environment allows us to better understand various aspects of adaptive behavior. This includes both components of the networks themselves in addition to its complex behavioral outputs. Much as with biological model organisms, their digital counterparts must allow for these processes to be experimentally tractable. The BV is a good model in this regard, since it allows for a realistic amount of complexity but also provides a means to reverse engineer this complexity.  

#### Suitability of BVs for Modeling Development
There are three benefits in choosing the Braitenberg vehicles paradigm to model development: a simplified structural-functional relationship, the ability to simulate an embodied nervous system, and the flexibility of modeling a heterogeneous population of agents. These benefits are summarized as follows

__Simplistic structure-function relationship__. BVs provide a simplistic model for understanding the interplay between the complexity of brains and their relationship to the physical world. 

__Embodied neural system__. BVs provide us with both a simple mapping between sensors and effectors. In addition, BVs allow us to model nervous system connectivity as a consequence of their behavior in the world. 

__Experimental precision__. Since the mappings between environmental input, nervous system elements, and behavioral outputs are fairly explicit, we can analyze populations of heterogeneous agents while minimizing potential experimental confounds.  

Neurodevelopment and Brain Networks
	The study of neural development has a long history [7]. The phenomenon of neural development proceeds from a simple group of cells to a complex and heterogeneous network of multiple functions. In vertebrates, for example, the spinal cord and brain arise from the neural tube and a subsequent process of neuronal differentiation [8]. In turn, the neural tube precursor is a sheet of undifferentiated cells. While this does not seem to be relevant to neural simulations of behavior, it does provide a means to demonstrate the emergence of network topology and its role in generating behavior [9]. This general drive towards complexity can be observed quite clearly in complex small connectomes with specialized function such as those found in Drosophila mushroom bodies [10]. BV models of development allow us to implement this drive towards complexity in a digital environment where the components of the emerging nervous system can be specified and measured. 

Even in the case of an in silico model, it is often difficult to approximate the complexity of a connectome. Attempts to grow a connectome in silico using the connection rules of an adult mouse brain [11] demonstrate the difficulty of simulating a network at large scales. While scale is a major factor in this complexity, the nature of developmental (as opposed to adult) rules are often unknown. There are, however, three principles that are derived from developmental processes and constraints: an expansion of the network, an adaptive specialization of the network, and resulting structural features that reflect function. All of these principles can be expressed to some degree using BV models, and are implicit in our software instantiations.

One unique aspect of the developmental connectome is the expanding neuronal network of embryogenesis. This network is first established in the embryo, and results from the differentiation of pluripotent cells into neural cells such as neurons, glia, and astrocytes. The genesis of biological neuronal networks can be divided into two steps: the birth of neurons, and the establishment of connections. Since neurons without connections are ultimately inviable, the birth of neurons and the establishment of physiochemical connections between these cells are necessary for plasticity in a connectome [12].

Another aspect of the developmental connectome is the selective elimination of connections during functional refinement. In terms of developmental plasticity, the greatest degree of connectivity occurs immediately following neurogenesis. This is due to evolutionarily conserved genetic mechanisms [13]. Once exposed to the environment, these connections are pruned so that only the most active connections remain. We see this type of pruning in the visual cortex during early life-history: as neural connections are exposed to the environment, they are reinforced [14]. In an artificial context, we expect that this will result in two different types of patterned connectivity in a biologically-inspired neural network. The first are hierarchical pathways centered on a few key cells, and the second involves connectivity between disparate sets of cells from a wide variety of nervous system regions [15]. This mix of hierarchical and distributed processing allows for many of the adaptive behaviors artificial neural systems are known for.

We can also observe hierarchical organization and disparate regional connectivity in biological organisms. Therefore, the third aspect of the developmental connectome is the structure of function resulting from developmental processes. During the process of growth and selection, a number of structural motifs emerge that are useful for robust function of the adult connectome. In C. elegans, the hierarchical nature of the connectome reveals a number of higher-order organization principles such as rich-club connectivity [16] and the hourglass effect [17]. These structural aspects have their origins in neural development, and in fact are the primary basis for facilitating functions such as developmental plasticity and learning.

The most obvious way we can model the developmental nervous system is to use a connectionist model. Yet connectionist models also imply a wider set of physical and computational properties. According to Farmer [18], connectionist models are dynamical systems with both interactions between variables explicitly constrained to a finite set of connections and fluid connections in terms of connective strength. In applications to development, it is this latter point that becomes highly relevant.

#### Development and Connectionism
One way to understand how the developmental process shapes the brain is to model development using customized connectionist or agent-based models. In Munakata & McClelland [19], connectionist models are shown as being useful in defining developmental trajectories, critical periods, and the ontogenetic learning process. The last of these (ontogenetic learning) can be defined as the developmental transition between innate processes that dominate in early development and learned mechanisms that take over later in development [20]. While not all species undergo this transition at the same rate (or even at all), computational models can generate a number of potential scenarios for this type of developmental plasticity.

Perhaps more generally, development is the process by which one level of performance or competence can lead to another [21]. As the neural substrate increases in size and complexity, the organism transitions to new behavioral regimes. These are expected to be expressed as either the modifications of previous states, or new states altogether. These behavioral regimes are the product of developmental constraints, evolutionary mechanisms, and environmental challenges to the organism [22]. In at least two examples, we can see that biological intelligence is a product of dynamical systems, not just the right set of connections between neurons. For example, the work of Rinaldi & Karmiloff-Smith [23] demonstrates that intelligence can fluctuate across the ontogenetic process, and is contingent upon both the genetics of development and environmental factors. Furthermore, it is demonstrated in [24] that so-called developmental transitions in reasoning behavior can be characterized as nonlinear dynamics and represented using a computational model. 

As the generation of complex behavior requires a more complete representation than afforded by a standard BV, we must also think in terms of representational complexity. A structural measure of representational complexity is introduced in Quartz & Sejnowski [25], and provides us with two foundational underpinnings for creating software instantiations of developmental BVs. First, development is defined by a progressive increase in representational complexity and associated anatomical structures. Secondly, increases in complexity corresponds to the interactions with the structural environment. While these points are consistent with the notion of neural constructivism [26], developmental BVs also require some inspiration from biological innateness. Consistent with the notion that the development process is a combination of learned experience and the unfolding of innate biological processes, Zador [27] argues that neural network simulations must include innate components in order to truly exploit the computational power of biological nervous systems. We add to the conventional literature on BVs in this respect: our instantiations incorporate hybrid representations (e.g. genetic/embodied) that exceed the traditional computational substrate of neural networks. 

We have also attempted to bridge the gap between strong biological fidelity and models of mixed cognitive and biological fidelity [28]. Such “mixed” models correspond to the deep learning/swarm instantiation presented in a later section. The other two software instantiations, in addition to the general computational developmental neuroscience model, exhibit strong biological fidelity. As such they rely on bottom-up organizational principles such as a plasticity of connections and the emergence of simple behaviors. On the other hand, mixed biological-cognitive models retain a pattern of connectivity throughout their life-history trajectory (and thus a non-plastic behavioral repertoire). Yet while each agent is used to represent singular behaviors, putting them in an environment with other agents representing the same or a multitude of behaviors can result in the observation of emergent phenomena [29].

#### Embodied Cognition
Braitenberg Vehicles offer an interesting opportunity to explore the embodiment of cognition. Embodied cognition [30-32] is an emerging approach that draws upon disciplines such as psychology, biology, cognitive science, robotics, and complex systems. Traditional views of cognition propose that the mind is not only a logical computational engine, but also operates independently of the external environment. An essential component of these computational systems are representations that can perform symbol-manipulation are [33]. Embodied cognition does not eschew representations, but views them as resulting from interactions between the organism and its environment.

Radical embodied cognition [34] explicitly rejects the role of representations, and posits that that cognition can to be described solely in terms of agent-environment dynamics. These interactions can then be understood through the application of quantitative techniques such as dynamical systems analysis. Embodied cognition challenges the notion that the sensory world and action in that world are peripheral or auxiliary elements of cognitive processes. According to the embodied cognition view, body and brain are interdependent in a way that enables us to approximate both the developmental process and a distributed nervous system with minimal representation [35, 36].

The use of developmental BVs is an attempt to reconcile low-level representations of the brain with both innate processes (evolution and development) and higher-level representations (multisensory integration). By examining the nature of how the modalities of sensory input influence perception, behavior, interpretation, or even representation, embodied cognition expands what is seen as integral to cognition [37]. Within the general notion of embodied cognition, there are differing perspectives about the degree of representation applicable or necessary. Radical embodied cognition investigates what elements of cognition, or perhaps proto- or pre-cognitive feedback loops, may operate with minimal if any representation or symbolic manipulation taking place [34].

Developmental BVs give us a unique opportunity: Braitenberg’s original conception of vehicles were embodied and representation-free models of simple internal structures that result in “intelligent” behaviors. As such, they offer a space to explore the innate and plastic components of the underlying developmental neurobiology; particularly regarding the potential range of expressed behaviors available for agents with minimal cognitive representation. Additionally, using developmental BVs may also provide greater perspective on the Neuroethology of developing individual and group behaviors: Graziano [38] suggests that a focus on the sensorimotor loop and the study of movement behaviors more generally is key to understanding cognition as a form of intentional action. 

#### Generalized Models of Regulation
Our approach relies upon several models of regulation that may also play a role in emergent nervous systems that interact with their environments. Our software instantiations present at least two: behavioral reinforcement and Hebbian learning. Behavioral reinforcement is most famously characterized through reinforcement learning techniques [39], but the core mechanism itself can be implemented using a host of other techniques [40]. For example, Hebbian learning is the dictum that “neurons that fire together, wire together” [41]. The co-occurence of particular neural units can produce spontaneous and adaptive behaviors depending on the context. Another example comes from the implementation of genetic algorithms, where fitness functions can serve to reinforce adaptive behaviors through hill-climbing on a fitness landscape [42]. More generally, our approaches involve a mechanism that allows for some form of adaptive feedback. Even in lower-capacity cognitive agents, a greater ability to model environmental conditions or interpret sensory-motor input may lend towards these agents developing into so-called good regulators [43]. This can be achieved through regulatory mechanisms for a single agent, or regulation of behaviors across multiple agents.

### Methods
This section presents the methods used to develop the software instantiations presented in the Results section. These include descriptions of software packages, and mathematical formalisms that describe each approach to our common problem.

#### BraGenBrain
The BraGenBrain approach utilizes a BV-genetic algorithm hybrid approach to produce adjacency matrices representing small connectomes. The use of operators such as crossover, mutation, and selection are used to introduce developmental plasticity, while the best performing developmental trajectories are discovered using natural selection. As the BV agents move around and interact in a sandbox simulation, agents develop both implicit (nervous system) as well as explicit (behavior) features. 

Environment and body. The BraGenBrain environment is a n-dimensional “box” of predefined size in pixels (which makes the suite screen size-independent) with so-called “world objects” across which the agents move. At the moment of writing, we have only conducted experiments in a two-dimensional space with one type of world objects defined as perfect circles of equal size. Yet extension of the program to additional dimensions is possible. An agent body incorporates many of the classic BV elements [5]. These include a body core, sensors that receive signals from world objects (the nature of these signals is defined later) as well as motors that move the whole body, that is body core, sensors and motors themselves based on the signal received by sensors (that is, sensors and motors are interconnected). Currently, we have conducted experiments with body of pre-defined “primitive Braitenberg” (rectangular) shape with sensors and motors circles small enough to mind them little in calculations, however the agent class (Vehicle) was supplied with a companion factory

__Table 1__. Constructing a vehicle in the BraGenBrain environment.

class Vehicle {                                                       
	//internal vehicle fields and methods
	...           
	companion object Factory {
		...                                                  
		fun simpleBVVehicle(...) : Vehicle {...}              
		//other vehicle-producing functions                   
		...                                                   
	}                                                             
}  
      
Creating a custom vehicle body is as easy as adding a new member function to the companion factory class. The world is filled with objects on pseudo-random position (Java Random generator) inside the box without taking into account closeness to already existing world objects.

Vehicles movement and attraction/repulsion. The simulation is being rendered in discrete “ticks” or “frames”. Movement of vehicles around the space is calculated each tick and is comprised of several steps. The first step in determining vehicle behavior is to approximate the effect of objects and the environment on the vehicle’s sensors. Any given set of objects has its own effect strength taken from the random distribution between two values, which can be configured in the BraGenBrain control panel. The force (F) generated by a vehicle can be calculated as

$${\Delta}{W_ij} = {\dfrac{\phi} {50 {W^2_ij}{I_j} + 1}} \tag{1}$$

where d is the distance between the world object and sensor measured in pixels, e is the effect strength (represented in code as effectStrength), and g is a specific gravity constant (approximated as g = 10). 

The second step is to determine vehicle movement. All speed vectors that result for each motor separately by propagating sensors signals are being aggregated together (in our experiments just summed up) and result in rotating movement and sliding movement. Rotating movement calculates the angle between two adjacent vectors using following formula 

$${\Delta}{W_ij} = {\dfrac{\phi} {50 {W^2_ij}{I_j} + 1}} \tag{2}$$

where x and y are the corresponding axes coordinates of two speed vectors: the previous time point sampled (tick-1) and the current time point sampled (tick0). When the vehicle body is rotated around the body’s midline, such sliding movements are calculated by adding updated tick speed vector lengths along the axes to the vehicle position after the previous tick. If the resulting angle () is negative, 2is added to rectify the value.

Brain Network Formalism. The neuronal network of our cognitive agents are represented as a directed acyclic graph (DAG), which can be defined in the form G = (V, E). In this formulation, Vis a set of neurons and E set of connections between neurons. Note that ∀i,jV |E(i,j)| 1, i.e. there can not be more edges than one between a distinct pair of nodes (but could be none).  

For the sake of balancing the signal of each neuron, we normalized the weights so each neuron output synapses weights sum up to one. We do this with a weight function defined as

$${\Delta}{W_ij} = {\dfrac{\phi} {50 {W^2_ij}{I_j} + 1}} \tag{3}$$

where a functionwwhich maps from the set of edges E to rational numbers between zero and one, so that the sum of weights of all edges from a particular network node is equal to 1. 

There is extreme overhead in storing each connection weight in a given network as a float (4 bytes). To overcome this, we compressed the representation to eight bits, so that there were 255 possible weights. This is defined by the following equation

$${\Delta}{W_ij} = {\dfrac{\phi} {50 {W^2_ij}{I_j} + 1}} \tag{4}$$

in which we multiply the w (rational weight) of a connection by the factor of 255 and take the next lower bound integer to this number (w<sub>concise</sub>). 

In this way, we preserve variety in the network while also reducing considerably the computational load. Related to this, we require a way to represent whole brain graphs in a compact manner. We chose matrix graph representation as a means to store connectivity information. This representation can be described mathematically as

$${\Delta}{W_ij} = {\dfrac{\phi} {50 {W^2_ij}{I_j} + 1}} \tag{5}$$

where each i,jth entry of matrix M is the weight of edge between nodes i and j in our brain only if i < j, and zero otherwise (dictated by the fact that we do not want to count the connection between i and j and then j and i twice).

__Code.__ BraGenBrain is written in Kotlin using the JavaFX-based TornadoFX for the GUI, TornadoFX being a somewhat more language-affine binding for Kotlin) and is optimized enough to run agent populations up to 1000 agents smoothly on a standard personal computer. For more information, please visit the home repository: https://github.com/Orthogonal-Research-Lab/ GSoC-Braitenberg-Vehicles/tree/master/Stefan

Modeling Neural Plasticity using Multisensory Inputs
This instantiation is to create a robust and efficient simulation of Hebbian plasticity in learning and memory. The simulation utilized a Braitenberg Vehicle (BV) that possesses an olfactory system (smell), a gustatory system (taste), an associative memory, a motor unit, and a judgement unit. The BV is allowed to explore freely in an environment where sources of olfactory and gustatory stimuli are distributed, and to learn the association between taste and odor so as to approach the good sources and avoid the bad.

__Environment Setup.__ In this instantiation, our developmental BV attempts to acquire the association between odor and taste in a 2-dimensional space of sources that emit odor and taste. We tried to make the environment realistic that olfactory stimuli decay with distances exponentially from their sources, while gustatory stimuli are sensible only when the BV is within gustatory boundaries of those stimuli. These can be represented using an odor space and a taste space (Figure 5), respectively [44]. The space can be expressed mathematically as

$${\Delta}{W_ij} = {\dfrac{\phi} {50 {W^2_ij}{I_j} + 1}} \tag{6}$$

where Ox,y,i is the ith olfactory feature sensible at position (x,y), and Io,i(k)is the ith feature of the odor omitted by stimulus source k; similarly, G and Ig,i (k)are for gustatory features. d d,y(k) is the Euclidean distance from (x,y) to source k, while dmax is the maximum distance in space, d′ is the gustatory sensible threshold, and c is an arbitrary scalar. Θ is the standard Heaviside function.

__Li-Hopfield Network.__ The olfactory system, is implemented as a type of Li-Hopfield network [45], which is used as a standard model of olfactory bulb function (Figure 6). Li-Hopfield networks model the dynamics of two important cells in olfactory bulb: mitral cells and granule cells. Mitral cells take in relayed sensory information from receptor cells and glomeruli as input, and produce appropriate outputs to other parts of the brain [46]. Meanwhile, granule cells serve as inhibitors of mitral cell activity [47]. In a biological context, the ratio of granule cells than mitral cells is high. In this model, however, there are equal numbers of each. The Li-Hopfield formalism can be described mathematically as

$${\Delta}{W_ij} = {\dfrac{\phi} {50 {W^2_ij}{I_j} + 1}} \tag{7}$$

where x and y are the internal states of mitral cells and granule cells. M, G, and L are the weight matrices from granule to mitral, mitral to granule, and mitral to mitral, respectively, f are activation functions, Γ is a function setting the lower triangular entries to zeros. I is the input and Icis the constant ("center") input, and  is the time constant. The powers of mitral cells’ oscillation are collected to be the input to the BV’s associative memory.

__Gustatory System.__ In this model, the gustatory system is only a single layer of cells, for taste is simply an "impression" in this simulation. There is no noise involved in taste [48], or any other perturbation, so further processing of taste is redundant [49]. 

__Associative Memory.__ To model the associative memory between odors and tastes, we implement an associative memory using the generalized Hebbian algorithm (GHA) [50] with depression. When only odor patterns are present, the associative memory maps the odor to taste (recalling mode). When odor and taste patterns coincide, the associative memory changes its weights with the following dynamics (learning mode)

$${\Delta}{W_ij} = {\dfrac{\phi} {50 {W^2_ij}{I_j} + 1}} \tag{8}$$

where W is the association between I'o, the processed olfactory input, and Ig, penalized using the depression matrix D with a depression rate . Wijis set to 0 if the denominator of Dijis zero. 

The latter two expressions are requisite for GHA to be stable. However, in this learning process, unlike typical machine learning with neural networks where samples are learned one by one, occurs in a space where samples are mixed and the time required to learn from each sample is unknown. The presentation order also presents unpredictability, as each sample can be revisited, either immediately or after the vehicle visits a sequence of samples. It is hard to determine the initial learning rate and control the pace of convergence, in addition to avoiding the effect of initial conditions when samples are not introduced serially. The learning rate is thus empirically set to a small constant (O(10-3)) and decreased by 1% after the BV gets a set of non-zero gustatory features. Moreover, the depression matrix that naively imitates activity-dependent long-term depression attempts to cancel the effect of repeatedly learning from one stimulus source and noisy data. Its effect can be demonstrated through static testing where the BV does not move and stimuli are presented without priming.

__Motor Unit.__ The motor unit is radian-based. The BV moves along the heading direction whose value is in [-, ]. When the increase in preference passes a threshold, the BV moves forward with a little offset based on the increase; when the decrease in preference passes the threshold, the BV moves backward with a little offset based on the decrease. Otherwise, it moves towards a nearby source. The motor unit is implemented in Movement.RadMotor class. Because the learning rate of GHA has to decrease to ensure stability, the motor unit is equipped with memory to avoid repeated back-and-forth movement near the gustatory boundary of a “good” sample, which could easily lead to overfitting.

__Simple Judgement Unit.__ The judgement of a source is based on its taste. A judgement unit can be defined in the following form

$${\Delta}{W_ij} = {\dfrac{\phi} {50 {W^2_ij}{I_j} + 1}} \tag{9}$$

where p is the preference from the summation of different judgement J on recalled taste I'g. If there is no recalled taste and real taste exists, then I'g=Ig. The preference, the output of the judgement unit, is the sum of the output of each preference function applied to their corresponding gustatory attributes. The judgement unit is incorporated in Simulation class.

__Code.__ This project uses Cython and C. The most time consuming parts are either written in core.c or implemented by using OpenBLAS.. Static images such as those shown above are produced through Networkx and Matplotlib, while real-time animation is generated using PyQtGraph and PyQt5. For more information, please visit the home repository: https://github.com/Orthogonal-Research-Lab/GSoC-Braitenberg-Vehicles/tree/master/Ziyi 

__Methods for swarm intelligence using BVs__
The idea here is to assign a set of rules associated with environmental stimuli to each agent (vehicle) as described in the book "Vehicles: Experiments in Synthetic Psychology" [5]. Based on these simple rules when multiple agents (vehicles) are introduced in the environment, they behave like intelligent swarms. In the present version, there is no interaction between the agents, so their behavior is solely dependent upon the nature of the stimuli present in the environment.

__Wiring and Activation Rules.__ The wiring and activation rules for the swarm intelligence approach can be defined considering functions for inhibition-dependent action 

$${\Delta}{W_ij} = {\dfrac{\phi} {50 {W^2_ij}{I_j} + 1}} \tag{10}$$  

where A1 is excitatory activation, A2is inhibited activation, r is the distance between sensor and stimulus and k, k1, k2are calibrated constants.

__Components of Vehicle Kinematics.__ As the sensory activation reaches the wheel motor, it induces two different angular velocities in the two wheels which is mainly responsible for the resulting vehicular movement. Considering the angular velocities of the two wheels and the vehicular dimensions, we can define the components of two dimensional motion of the vehicle as

$${\Delta}{W_ij} = {\dfrac{\phi} {50 {W^2_ij}{I_j} + 1}} \tag{11}$$  

where Vx and Ay are the components of linear velocity along the axis of the vehicle and the radial acceleration perpendicular to the axis, respectively. 1, 2 are the angular velocities of left and right wheel respectively, while k is the calibrated constant depending upon vehicular dimensions. Based on an egocentric view of the environment, the vehicle can take either a left or right turn depending upon the difference in angular velocities. 1< 2results in a turn leftward, while 1> 2 results in a turn rightward 

Code. The code for this instantiation has been constructed in the Processing.py software environment (written in Python). For the elementary setup option, opt for python-mode. Alternatively, you can run this program in terminal as well as edit the code in your favorite text editor. For more information, please visit the home repository: [https://github.com/ankiitgupta7/ Simulations- of-Braitenberg-Vehicles](https://github.com/ankiitgupta7/ Simulations- of-Braitenberg-Vehicles)


### Results

#### Computational model of Developmental Neuroscience
One goal of this project is to create a generalized computational model of neurodevelopment. This will allow us to investigate a large number of potential research questions. In general, we have found that there are three ways to approach an approximation of development and plasticity. Two of these approaches are a forward mapping, and the third is an inverse mapping. The first is to use a correlation (or covariance) matrix approach, where all neural units in the nervous system are compared with every other neural unit. This results in pairwise comparisons that can lead to connectome network maps [51]. The second approach is to add nodes and arcs sequentially to a simple set of I/O connections. In this case, we get a more explicit network topology, and can observe phenomena such as preferential attachment. A third approach is to prune connections from a fully-formed network engaged in hard-wired behaviors. Using this approach, one can come to understand exactly which connections and neurons are essential for the execution of a behavior.



#### Software Instantiations.
Currently, there are three software instantiations of the software: BraGenBrain, modeling neural plasticity using multisensory inputs, and so-called BVs as Deep Learning. We will first discuss the details and limitations of the BraGenBrain approach, then do the same for the multisensory plasticity approach, and finally discuss the collective BVs and their cumulative behavior. 
 
__BraGenBrain: a genetic algorithmic approach.__ The first instantiation is the BraGenBrain approach, which incorporates a genetic algorithm as the basis for neuronal development in a population of BV agents. A sample control panel and environment for BraGenBrain are shown as screenshots in Figures 1 and 2, respectively. The genetic algorithm generates new developmental variants, which in turn have a certain level of performance in a pre-defined environment.  

<p align="center">
  <img width="600" height="300" src="https://github.com/Orthogonal-Research-Lab/GSoC-Braitenberg-Vehicles/blob/master/Manuscript/fig-4.png"><BR>
Figure 1. Control panel for BraGenBrain. Properties that can be modified include the number of objects in the world, two-dimensional size of the world, genetic algorithm parameters, and properties of the vehicles (agents).
</p>

<p align="center">
  <img width="600" height="300" src="https://github.com/Orthogonal-Research-Lab/GSoC-Braitenberg-Vehicles/blob/master/Manuscript/fig-4.png"><BR>
Figure 2. Screenshot of BraGenBrain environment. LEFT: spatial array with environmental features (black dots) and agents (red rectangles). RIGHT: bivariate plot of values, values for each agent are shown as red dots.
</p>

Each of these developmental variants represents a distinct DAG, and can be defined as different topological orderings for each set of nodes in the DAG. In this way, we were able to retrieve a binary string representation of a brain (graph) by just taking its matrix representation and encode any number with 8 bits. 

__Agent Nervous System Development.__ Figure 3 shows a schematic representation of the agent’s properties as a directed network. The root nodes of the network consists of sensors. Here, signals from the environment enter the network. The leaves of the network are represented by motors, which take inputs from the signal vectors generated by the inputs and intermediate layers. We also use a three-step evolutionary algorithm to simulate nervous system development. In the first step, mutation, random position in the binary brain representation is being mutated (i.e. bit-flipped) with a configurable probability, and a new vehicle with this innovation is being “released” in the world. 

<p align="center">
  <img width="600" height="300" src="https://github.com/Orthogonal-Research-Lab/GSoC-Braitenberg-Vehicles/blob/master/Manuscript/fig-4.png"><BR>
Figure 3. Schematic representation of BraGenBrain agent brain graph. Sensors and motors are connected through synapses and internal layer nodes.
</p>

In the subsequent step, selection keeps vehicles who exhibit the most variable movement (defining by calculating the area under their three last speed vectors) and some lucky ones with a certain probability. The third step is crossover, defined by randomly picking two already existing individuals (decided by looping over all possible pairs and throwing a coin with a mating rate chance), cutting their brain representations into two pieces on a random place and then rejoining those gametes together to produce a new brain representation. The offspring is released in the world with a new brain and a default body phenotype. The latter is customizable.

Modeling Neural Plasticity using Multisensory Inputs
	We implemented a multisensory Braitenberg vehicle and visualized its behaviors and association development during its exploration in an environment (Figure 4, 5, and 6). In all simulations (n > 30) we ran, the BV successfully associated taste with smell to some degree when both taste and smell information are available. When there is no taste, it recalled the taste based on its associative memory and the smell received. In both cases where tastes were actually sensed or recalled, it can have preference on the source it is approaching, and then detour if the preferences were low. When the BV becomes more and more mature via association, it can exhibit significant avoidance and preference behaviors, in a manner similar to small animals. An example of the simulation is shown in Figure 4.

<p align="center">
  <img width="600" height="300" src="https://github.com/Orthogonal-Research-Lab/GSoC-Braitenberg-Vehicles/blob/master/Manuscript/fig-4.png"><BR>
Figure 4. An example of real-time animation. Experiments are conducted using an iPython Jupyter Notebook. Left large panel: the BV (red) exploring in a space of sources (blue) emitting odor and taste patterns. Right grid, top left: the actual preference (calculated from the recalled taste pattern) compared with the ideal preference (calculated from the ideal taste pattern). Top right: the output of olfactory bulb (Li-Hopfield model). Bottom left: the received odor pattern, and the recalled and ideal taste patterns. Bottom right: a heat map of the developing association matrix. Note that the ideal taste patterns are the output of our designed mapping used to generate the spaces (Figure 5) which is not known by the BV.
</p>

We also investigated why the BVs in our instantiation were able to make associations and recall tastes from odors when the stimuli were mixed. Through visualization, we observed that it was able to alter its oscillatory frequencies based on changes in olfactory attributes with small latency, so that it filtered out much of the background odor and allowed the BV to identify specifically the odor of the stimulus source the BV is approaching. 

<p align="center">
  <img width="600" height="300" src="https://github.com/Orthogonal-Research-Lab/GSoC-Braitenberg-Vehicles/blob/master/Manuscript/fig-4.png"><BR>
Figure 5. Odor space of one olfactory attribute (left) and taste space of one gustatory attribute (right). Number of sources = 20.  
</p>

<p align="center">
  <img width="600" height="300" src="https://github.com/Orthogonal-Research-Lab/GSoC-Braitenberg-Vehicles/blob/master/Manuscript/fig-4.png"><BR>
Figure 6. LEFT. Li-Hopfield model of the olfactory bulb. Mitral cells (grey) have excitatory projections (red arrow) to nearby mitral cells and granule cells (black). Granule cells simply inhibit (blue arrow) mitral cells. RIGHT. An example of an associative memory network.  
</p>

The general capacity for associating odor and taste patterns using this BV architecture was also assessed. We changed the whole space (i.e. the sources and their distribution) after every 104 steps, which were sufficient to let the BV touch nearly every source at least once. After two changes, the BV significantly lost its recallability in the new space, and manifested poor avoidance to unpreferred sources. We determined from the association matrix that it was due to the instability of our algorithm. Indeed, when the number of unique sources to learn, the order of encountering them, and the times of approaching them are all unclear, it is hard to find an optimal dynamics of the variable learning rate. Moreover, associative memories also have certain capacities that limit the amount of pattern they can encode [52].

BV Collectives
In [5], Braitenberg focuses on a bottom-up approach to synthesize emergent behaviours. One central theme of the book is that synthesis is easier than analysis. Essentially, it is easier to mimic behaviour rather than build the true underlying mechanism that is creating that behaviour. Each type of vehicle is characterized by these two properties being unique to them. Tuning these properties leads to different emergent behaviors. As a result, different names are assigned to the vehicles. The vehicle movement is particularly influenced by sensory activation received through its sensors and how this activation is transferred to the motor through internal wiring. 

While individual agent behavior is innate (hard-coded), multiple agents placed into the environment simultaneously respond to common stimuli by exhibiting a particular type of behavior. Figures 7 and 8 demonstrate a collective of a particular vehicle type and stimulus combination. In Figure 7, a collective of 2a vehicle types is presented with a fixed stimulus. By contrast, Figure 8 demonstrates 2b vehicle collectives presented with a moving stimulus. Each stimulus-vehicle type combination seems to produce its own set of group behaviors. While we do not present an analysis of these behaviors here, they nonetheless seem to exhibit properties consistent with emergent self-organization.

Linear and Nonlinear Behavior. Creating something that acts complex is easier than analysing what looks like a complex system. An excitatory activation, A1 is used in the case of 1A, 2A, 2B, while an inhibitory activation A2 is used in the case of 1B, 3A, 3B. While monotonic functions are used for Vehicle types 1 through 3, Vehicle type 4 exhibits non-monotonic activation, as a mixture of inhibitory and excitatory connections are used. Table 2 shows the relationship between vehicle type, connectivity type, and associated emotions. 

<p align="center">
  <img width="600" height="300" src="https://github.com/Orthogonal-Research-Lab/GSoC-Braitenberg-Vehicles/blob/master/Manuscript/fig-4.png"><BR>
Figure 7. Screenshot of Vehicle 2a (coward) with fixed stimuli.
</p>

<p align="center">
  <img width="600" height="300" src="https://github.com/Orthogonal-Research-Lab/GSoC-Braitenberg-Vehicles/blob/master/Manuscript/fig-4.png"><BR>
Figure 8 Screenshot of Vehicle 2b (aggressive) with moving stimuli (movement not shown).
</p>

In terms of internal wiring, these simulations exhibit four combinations between the pair of sensors and the wheels. For each internal connection between a sensor and a wheel, there is an identity that corresponds to a weight. The weights themselves are binary (based on a threshold), and are different than probabilistic weights (based on a distribution). For lateral connections {w1, w2, w3, w4}, the weights would be {1, 1, 0, 0}. In the case of cross-talk (four contralateral connections), the weights corresponding to {w1, w2, w3, w4} would be {1, 1, 0, 0}. This can be clearly seen in the implemented code.

Table 2. Available experimental conditions to demonstrate collective BVs behaviors.
Stimulus Dynamics | Vehicle Type | Associated Emotion | Connectivity |
Fixed | 1a | Alive | Excitatory |
Moving | 1a | Alive | Excitatory |
Fixed | 1b | Alive | Inhibitory |
Moving | 1b | Alive | Inhibitory |
Fixed | 2a | Coward | Excitatory |
Moving | 2a | Coward | Excitatory |
Fixed | 2b | Aggressive | Excitatory |
Moving | 2b | Aggressive | Excitatory |
Fixed | 3a | Love | Inhibitory |
Moving | 3a | Love | Inhibitory |
Fixed | 4a | Instinct | Inhibitory/Excitatory |
Moving | 4a | Instinct | Inhibitory/Excitatory |
Fixed | 4b | Decision-making | Inhibitory/Excitatory |
Moving | 4b | Decision-making | Inhibitory/Excitatory |











 
_Environment._ The environment is realistic that olfactory stimuli decay with distances exponentially from their sources, while gustatory stimuli are sensible only when the BV is within gustatory boundaries of those stimuli. The mapping between olfactory attributes and gustatory attributes should be defined before initializing Space.Space class and Simulation class. These outputs can be represented using an odor space and a taste space, respectively (Figure 2).

<p align="center">
  <img width="345" height="323" src="https://github.com/Orthogonal-Research-Lab/GSoC-Braitenberg-Vehicles/blob/master/Manuscript/fig-3-left.png"><img width="345" height="323" src="https://github.com/Orthogonal-Research-Lab/GSoC-Braitenberg-Vehicles/blob/master/Manuscript/fig-3-right.png"><BR>
  Figure 2. Odor space [20] of one olfactory attribute (left) and taste space of one gustatory attribute (right).
</p>

_Olfactory System._ The olfactory system, is implemented as a type of Li-Hopfield network [21], which is used as a standard model of olfactory bulb function (Figure 3). Li-Hopfield networks model the dynamics of two important cells in olfactory bulb: mitral cells and granule cells. Mitral cells take in relayed sensory information from receptor cells and glomeruli as input, and produce appropriate outputs to other parts of the brain [22]. Meanwhile, granule cells serve as inhibitors of mitral cell activity [23]. In a biological context, the ratio of granule cells than mitral cells is high. In this model, however, there are equal numbers of each.

<p align="center">
  <img width="450" height="450" src="https://github.com/Orthogonal-Research-Lab/GSoC-Braitenberg-Vehicles/blob/master/Manuscript/fig-1.png"><BR>
  Figure 3. Li-Hopfield-inspired model of the olfactory bulb. The grey dots are the mitral cells and the black are the granule cells. Red means excitation and blue means inhibition.
</p>

The Li-Hopfield network has been characterized as a group of coupled nonlinear oscillators [24]. In short, it is able to alter its oscillatory frequencies based on changes in olfactory attributes, so it is important to "filter" the noise and identify which stimulus source the BV is approaching. The signal powers of the output are then calculated, instead of modeling a complex afferent nerve in real nervous system. The olfactory system is implemented in Layers.LiHopfield class.

_Gustatory System._ In this model, the gustatory system is only a single layer of cells, for taste is simply an "impression" in this simulation. There is no noise involved in taste [25], or any other perturbation, so further processing of taste is redundant [26]. The gustatory system is implemented in Layers.Single class.

_Associative Memory._ The associative memory, implemented as a bidirectional associative memory (BAM), is how Hebbian learning is represented in this model (Figure 4). Rather than Hebbian rule that BAM often utilized, a Generalized Hebbian algorithm (GHA) is used, for it is demonstrably stable. The learning rate converges to zero with a constant rate to ensure the stability of GHA.

<p align="center">
  <img width="450 height="450" src="https://github.com/Orthogonal-Research-Lab/GSoC-Braitenberg-Vehicles/blob/master/Manuscript/fig-2.png"><BR>
  Figure 4. An example of a bidirectional associative memory (BAM) network. Nodes on the left are input cells, and the nodes on the right are output cells.
</p>

_Motor Unit._ The motor unit is radian-based. The BV moves along the heading direction whose value is in [-, ]. When the increase in preference passes a threshold, the BV moves forward with a little offset based on the increase; when the decrease in preference passes the threshold, the BV moves backward with a little offset based on the decrease. Otherwise, it moves towards a nearby source. The motor unit is implemented in Movement.RadMotor class. Because the learning rate of GHA has to decrease to ensure stability, the motor unit is equipped with memory to avoid repeated back-and-forth movement near the gustatory boundary of a “good” sample, which could easily lead to overfitting.

_Judgement Unit._ An array of preference function should be defined before initializing Simulation class. The preference, the output of the judgement unit, is the sum of the output of each preference functions applied to their corresponding gustatory attributes. The judgement unit is incorporated in Simulation class.


__BVs as a Deep Learning/Swarm model.__

### Discussion and Future Plans

Modeling Neural Plasticity using Multisensory Inputs. Future Plan. Put the trained BV in a new, testing environment, like conducting tests in animal models. Implement the progress saving functionality.



#### Use cases


Possible Implementation for modeling neural plasticity using multisensory inputs include more complex senses or more than two senses. More than 1 BV and BVs interactions. Another possible solution: apply genetic algorithm or other kinds to optimize the network structure.


### References
[1] Kaiser, M. (2017). Mechanisms of Connectome Development. _Trends in Cognitive Sciences_, 21(9), P703-P717. doi:10.1016/j.tics.2017.05.010.

[2]  Alicea, B. (2017). The Emergent Connectome in Caenorhabditis elegans Embryogenesis. _BioSystems_, 173, 247-255. 

[3] Larson, S.D., Gleeson, P., & Brown, A.E.X. (2018). Connectome to behaviour: modelling Caenorhabditis elegans at cellular resolution. _Philosophical Transactions of the Royal Society of London B_, 373(1758), 20170366. doi:10.1098/rstb.2017.0366.

[4] Fan, X. & Markram, H. (2019). A Brief History of Simulation Neuroscience. _Frontiers in Neuroinformatics_, doi:10.3389/fninf.2019.00032.

[5] Braitenberg, V. (1984). Vehicles: experiments in synthetic Psychology. MIT Press, Cambridge, MA.

[6] Seung, S. (2009). Connectome: How the Brain's Wiring Makes Us Who We Are. Houghton-Mifflin, Boston.

[7] Stiles, J. & Jernigan, T.L. (2010). The Basics of Brain Development. _Neuropsychological Review_, 20(4), 327–348. doi:10.1007/s11065-010-9148-4.

[8] Smith, J.L. & Schoenwolf, G.C. (1997). Neurulation: coming to closure. _Trends in Neurosciences_, 20(11), P510-P517. doi:10.1016/S0166-2236(97)01121-1.

[9] Eichler, K., Li, F., Litwin-Kumar, A., Park, Y., Andrade, I., Schneider-Mizell, C.M., Saumweber, T., Huser, A., Eschbach, C., Gerber, B., Fetter, R.D., Truman, J.W., Priebe, C.E., Abbott, L.F., Thum, A.S., Zlatic, M., & Cardona, A. (2017). The complete connectome of a learning and memory centre in an insect brain. _Nature_, 548(7666), 175-182. doi:10.1038/nature23455

[10] Craddock, R.C., Tungaraza, R.L., & Milham, M.P. (2015). Connectomics and new approaches for analyzing human brain functional connectivity. _Gigascience_, 4, 13.

[11] Towlson, E.K., Vertes, P.E., Ahnertm, S.E., Schafer, W.R., & Bullmore, E.T. (2013). The rich club of the C. elegans neuronal connectome. _Journal of Neuroscience_, 33(15), 6380-6387. doi:10.1523/JNEUROSCI.3784-12.2013.

[12] Sabrin, K.M & Dovrolis, C. (2017). The hourglass effect in hierarchical dependency networks. _Network Science_, 5(4), 490-528.

[13] Kriegeskorte, N. & Douglas, P.K. (2018). Cognitive computational neuroscience. _Nature Neuroscience_, 21, 1148–1160.

[14] Couzin, I. & Krause, J. (2003). Self-Organization and Collective Behavior in Vertebrates. _Advances in the Study of Behavior_, 32, 1-75 doi:10.1016/S0065-3454(03)01001-5.

[15] Neftci, E.O. & Averbeck, B.B. (2019). Reinforcement learning in artificial and biological systems. _Nature Machine Intelligence_, 1, 133–143. 

[16] Drugan, M.M. (2019). Reinforcement learning versus evolutionary computation: A survey on hybrid algorithms. _Swarm and Evolutionary Computation_, 44, 228-246.

[17] Munakata, Y. & Pfaffly, J. (2004). Hebbian learning and development. _Developmental Science_, 7(2), 141–148.

[18] Khaluf, Y., Ferrante, E., Simoens, P., & Huepe, C. (2017). Scale invariance in natural and artificial collective systems: a review. _Journal of the Royal Society Interface_, 14, 20170662. doi:10.1098/rsif.2017.0662.

[19] Henriksen, S., Pang, R., & Wronkiewicz, M. (2016). A simple generative model of the mouse mesoscale connectome. _eLife_, 5, e12366. doi:10.7554/eLife.12366.

[20] Soh, Z., Nishikawa, S., Kurita, Y., Takiguchi, N., & Tsuji, T. (2016). A Mathematical Model of the Olfactory Bulb for the Selective Adaptation Mechanism in the Rodent Olfactory System. _PLoS One_, 11(12), e0165230. doi: 10.1371/journal.

[21] Li, Z. & Hopfield, J.J. (1989). Modeling the Olfactory Bulb and its Neural Oscillatory Processings. _Biological Cybernetics_, 61, 379-392.

[22] Nagayama, S., Enerva, A., Fletcher, M. L., Masurkar, A.V., Igarashi, K.M., Mori, K., & Chen, W.R. (2010). Differential axonal projection of mitral and tufted cells in the mouse main olfactory system. _Frontiers in Neural Circuits_, 4(120). doi: 10.3389/fncir.2010.00120.

[23] Shepherd, G.M., Chen, Willhite, W.R. D., Migliore, M., & Greer C.A. (2007). The olfactory granule cell: From classical enigma to central role in olfactory processing. _Brain Research Reviews_, 55, 373-382.

[24] Sanger, T. D. (1989). Optimal Unsupervised Learning in a Single-Layer Linear Feedforward Neural Network. _Neural Networks_, 2, 459-473.

[25] Smith, D.V. & St John, S.J. (1999). Neural coding of gustatory information. _Current Opinion in Neurobiology_, 9, 427-435.

[26] Wu, A., Dvoryanchikov, G., Pereira, E., Chaudhari, N., & Roper, S. D. (2015). Breadth of tuning in taste afferent neurons varies with stimulus strength. _Nature Communications_, 6, 8171. doi: 10.1038/ncomms9171.


