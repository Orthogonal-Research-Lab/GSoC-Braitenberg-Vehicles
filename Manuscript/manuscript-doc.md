_Please install [MathJax Plugin for Github](https://chrome.google.com/webstore/detail/mathjax-plugin-for-github/ioemnmodlmafdkllaclgeombjnmnbima/related) or [Math Anywhere](https://chrome.google.com/webstore/detail/math-anywhere/gebhifiddmaaeecbaiemfpejghjdjmhc) Chrome plug-ins to view equations_  

## Braitenberg Vehicles as Developmental Neurosimulation
Bradly Alicea<sup>1,2</sup>, Stefan Dvoretskii<sup>3</sup>, Ziyi Gong<sup>4</sup>, Ankit Gupta<sup>5</sup>, and Jesse Parent<sup>6</sup>

<sup>1</sup> Orthogonal Research and Education Laboratory, <sup>2</sup> OpenWorm Foundation, <sup>3</sup> Technische Universität München, <sup>4</sup> University of Pittsburgh, <sup>5</sup> IIT Kharagpur, <sup>6</sup> SUNY Albany.

### Abstract
The connection between brain and behavior is a longstanding issue in the areas of  behavioral science, artificial intelligence, and neurobiology. Particularly in artificial intelligence research, behavior is generated by a black box approximating the brain. As is standard among models of artificial and biological neural networks, an analogue of the fully mature brain is presented as a blank slate. This model generates outputs and behaviors from a priori associations, yet this does not consider the realities of biological development and developmental learning. Our purpose is to model the development of an artificial organism that exhibits complex behaviors. We will introduce our approach, which is to use Braitenberg Vehicles (BVs) to model the development of an artificial nervous system. The resulting developmental BVs will generate behaviors that range from stimulus responses to group behavior that resembles collective motion. Next, we will situate this work in the domain of artificial brain networks. Then we will focus on broader themes such as embodied cognition, feedback, and emergence. Our perspective will then be exemplified by three software instantiations that demonstrate how a BV-genetic algorithm hybrid model, multisensory Hebbian learning model, and multi-agent approaches can be used to approach BV development. We introduce use cases such as optimized spatial cognition (vehicle-genetic algorithm hybrid model), hinges connecting behavioral and neural models (multisensory Hebbian learning model), and cumulative classification (multi-agent approaches). In conclusion, we will revisit concepts related to our approach and how they might guide future development.  

### Introduction
How do we understand the emergence of a connected nervous system, particularly in terms of how it leads to neural function and behavior? One way is to infer the co-occurrence of neural cell differentiation in a model organism [1,2]. This requires a small connectome in which cell differentiation can be tracked. Even for organisms such as the nematode _Caenorhabditis elegans_ [3], direct experimentation is difficult. An embodied _in silico_ system with a generalized nervous system would provide a means to both modify the developmental process and directly observe all possible developmental outcomes. Utilizing an abstraction to study hard-to-observe questions is in fact consistent with how theoretical modeling and simulations have been used throughout the history of neuroscience [4]. We propose that Braitenberg Vehicles (BV) [5] can be used as a means to construct such simulations. Originally proposed by Valentino Braitenberg, BVs are an embodied model of a simple nervous system. The minimalist architecture allows us to focus on the connection between an embodied  connectome and its behavioral outputs. It is of note that Braitenberg’s original vehicles were designed as thought experiments to show how seemingly complex behaviors may emerge from hard-wired nervous systems and phenotypes. Our approach differs in that we allow nervous systems to develop using a variety of techniques. We will introduce a general computational model, followed by specific instantiations involving different aspects of cognition.  

#### Motivation
This work is motivated by a desire to understand neurodevelopment balanced with a need to establish an in silico model system that allows us to simulate processes such as learning, plasticity, and the regulation of behavior. Of particular interest is a model which allows us to model global structures such as the connectome [6]. An artificial connectome that develops in the context of a controlled environment allows us to better understand various aspects of adaptive behavior. This includes both components of the networks themselves in addition to its complex behavioral outputs. Much as with biological model organisms, their digital counterparts must allow for these processes to be experimentally tractable. The BV is a good model in this regard, since it allows for a realistic amount of complexity but also provides a means to reverse engineer this complexity.  

#### Suitability of BVs for Modeling Development
There are three benefits in choosing the Braitenberg vehicles paradigm to model development: a simplified structural-functional relationship, the ability to simulate an embodied nervous system, and the flexibility of modeling a heterogeneous population of agents. These benefits are summarized as follows

__Simplistic structure-function relationship__. BVs provide a simplistic model for understanding the interplay between the complexity of brains and their relationship to the physical world. 

__Embodied neural system__. BVs provide us with both a simple mapping between sensors and effectors. In addition, BVs allow us to model nervous system connectivity as a consequence of their behavior in the world. 

__Experimental precision__. Since the mappings between environmental input, nervous system elements, and behavioral outputs are fairly explicit, we can analyze populations of heterogeneous agents while minimizing potential experimental confounds.  

Neurodevelopment and Brain Networks
	The study of neural development has a long history [7]. The phenomenon of neural development proceeds from a simple group of cells to a complex and heterogeneous network of multiple functions. In vertebrates, for example, the spinal cord and brain arise from the neural tube and a subsequent process of neuronal differentiation [8]. In turn, the neural tube precursor is a sheet of undifferentiated cells. While this does not seem to be relevant to neural simulations of behavior, it does provide a means to demonstrate the emergence of network topology and its role in generating behavior [9]. This general drive towards complexity can be observed quite clearly in complex small connectomes with specialized function such as those found in Drosophila mushroom bodies [10]. BV models of development allow us to implement this drive towards complexity in a digital environment where the components of the emerging nervous system can be specified and measured. 

Even in the case of an in silico model, it is often difficult to approximate the complexity of a connectome. Attempts to grow a connectome in silico using the connection rules of an adult mouse brain [11] demonstrate the difficulty of simulating a network at large scales. While scale is a major factor in this complexity, the nature of developmental (as opposed to adult) rules are often unknown. There are, however, three principles that are derived from developmental processes and constraints: an expansion of the network, an adaptive specialization of the network, and resulting structural features that reflect function. All of these principles can be expressed to some degree using BV models, and are implicit in our software instantiations.

One unique aspect of the developmental connectome is the expanding neuronal network of embryogenesis. This network is first established in the embryo, and results from the differentiation of pluripotent cells into neural cells such as neurons, glia, and astrocytes. The genesis of biological neuronal networks can be divided into two steps: the birth of neurons, and the establishment of connections. Since neurons without connections are ultimately inviable, the birth of neurons and the establishment of physiochemical connections between these cells are necessary for plasticity in a connectome [12].

Another aspect of the developmental connectome is the selective elimination of connections during functional refinement. In terms of developmental plasticity, the greatest degree of connectivity occurs immediately following neurogenesis. This is due to evolutionarily conserved genetic mechanisms [13]. Once exposed to the environment, these connections are pruned so that only the most active connections remain. We see this type of pruning in the visual cortex during early life-history: as neural connections are exposed to the environment, they are reinforced [14]. In an artificial context, we expect that this will result in two different types of patterned connectivity in a biologically-inspired neural network. The first are hierarchical pathways centered on a few key cells, and the second involves connectivity between disparate sets of cells from a wide variety of nervous system regions [15]. This mix of hierarchical and distributed processing allows for many of the adaptive behaviors artificial neural systems are known for.

We can also observe hierarchical organization and disparate regional connectivity in biological organisms. Therefore, the third aspect of the developmental connectome is the structure of function resulting from developmental processes. During the process of growth and selection, a number of structural motifs emerge that are useful for robust function of the adult connectome. In C. elegans, the hierarchical nature of the connectome reveals a number of higher-order organization principles such as rich-club connectivity [16] and the hourglass effect [17]. These structural aspects have their origins in neural development, and in fact are the primary basis for facilitating functions such as developmental plasticity and learning.

The most obvious way we can model the developmental nervous system is to use a connectionist model. Yet connectionist models also imply a wider set of physical and computational properties. According to Farmer [18], connectionist models are dynamical systems with both interactions between variables explicitly constrained to a finite set of connections and fluid connections in terms of connective strength. In applications to development, it is this latter point that becomes highly relevant.

#### Development and Connectionism
One way to understand how the developmental process shapes the brain is to model development using customized connectionist or agent-based models. In Munakata & McClelland [19], connectionist models are shown as being useful in defining developmental trajectories, critical periods, and the ontogenetic learning process. The last of these (ontogenetic learning) can be defined as the developmental transition between innate processes that dominate in early development and learned mechanisms that take over later in development [20]. While not all species undergo this transition at the same rate (or even at all), computational models can generate a number of potential scenarios for this type of developmental plasticity.

Perhaps more generally, development is the process by which one level of performance or competence can lead to another [21]. As the neural substrate increases in size and complexity, the organism transitions to new behavioral regimes. These are expected to be expressed as either the modifications of previous states, or new states altogether. These behavioral regimes are the product of developmental constraints, evolutionary mechanisms, and environmental challenges to the organism [22]. In at least two examples, we can see that biological intelligence is a product of dynamical systems, not just the right set of connections between neurons. For example, the work of Rinaldi & Karmiloff-Smith [23] demonstrates that intelligence can fluctuate across the ontogenetic process, and is contingent upon both the genetics of development and environmental factors. Furthermore, it is demonstrated in [24] that so-called developmental transitions in reasoning behavior can be characterized as nonlinear dynamics and represented using a computational model. 

As the generation of complex behavior requires a more complete representation than afforded by a standard BV, we must also think in terms of representational complexity. A structural measure of representational complexity is introduced in Quartz & Sejnowski [25], and provides us with two foundational underpinnings for creating software instantiations of developmental BVs. First, development is defined by a progressive increase in representational complexity and associated anatomical structures. Secondly, increases in complexity corresponds to the interactions with the structural environment. While these points are consistent with the notion of neural constructivism [26], developmental BVs also require some inspiration from biological innateness. Consistent with the notion that the development process is a combination of learned experience and the unfolding of innate biological processes, Zador [27] argues that neural network simulations must include innate components in order to truly exploit the computational power of biological nervous systems. We add to the conventional literature on BVs in this respect: our instantiations incorporate hybrid representations (e.g. genetic/embodied) that exceed the traditional computational substrate of neural networks. 

We have also attempted to bridge the gap between strong biological fidelity and models of mixed cognitive and biological fidelity [28]. Such “mixed” models correspond to the deep learning/swarm instantiation presented in a later section. The other two software instantiations, in addition to the general computational developmental neuroscience model, exhibit strong biological fidelity. As such they rely on bottom-up organizational principles such as a plasticity of connections and the emergence of simple behaviors. On the other hand, mixed biological-cognitive models retain a pattern of connectivity throughout their life-history trajectory (and thus a non-plastic behavioral repertoire). Yet while each agent is used to represent singular behaviors, putting them in an environment with other agents representing the same or a multitude of behaviors can result in the observation of emergent phenomena [29].

#### Embodied Cognition
Braitenberg Vehicles offer an interesting opportunity to explore the embodiment of cognition. Embodied cognition [30-32] is an emerging approach that draws upon disciplines such as psychology, biology, cognitive science, robotics, and complex systems. Traditional views of cognition propose that the mind is not only a logical computational engine, but also operates independently of the external environment. An essential component of these computational systems are representations that can perform symbol-manipulation are [33]. Embodied cognition does not eschew representations, but views them as resulting from interactions between the organism and its environment.

Radical embodied cognition [34] explicitly rejects the role of representations, and posits that that cognition can to be described solely in terms of agent-environment dynamics. These interactions can then be understood through the application of quantitative techniques such as dynamical systems analysis. Embodied cognition challenges the notion that the sensory world and action in that world are peripheral or auxiliary elements of cognitive processes. According to the embodied cognition view, body and brain are interdependent in a way that enables us to approximate both the developmental process and a distributed nervous system with minimal representation [35, 36].

The use of developmental BVs is an attempt to reconcile low-level representations of the brain with both innate processes (evolution and development) and higher-level representations (multisensory integration). By examining the nature of how the modalities of sensory input influence perception, behavior, interpretation, or even representation, embodied cognition expands what is seen as integral to cognition [37]. Within the general notion of embodied cognition, there are differing perspectives about the degree of representation applicable or necessary. Radical embodied cognition investigates what elements of cognition, or perhaps proto- or pre-cognitive feedback loops, may operate with minimal if any representation or symbolic manipulation taking place [34].

Developmental BVs give us a unique opportunity: Braitenberg’s original conception of vehicles were embodied and representation-free models of simple internal structures that result in “intelligent” behaviors. As such, they offer a space to explore the innate and plastic components of the underlying developmental neurobiology; particularly regarding the potential range of expressed behaviors available for agents with minimal cognitive representation. Additionally, using developmental BVs may also provide greater perspective on the Neuroethology of developing individual and group behaviors: Graziano [38] suggests that a focus on the sensorimotor loop and the study of movement behaviors more generally is key to understanding cognition as a form of intentional action. 

#### Generalized Models of Regulation
Our approach relies upon several models of regulation that may also play a role in emergent nervous systems that interact with their environments. Our software instantiations present at least two: behavioral reinforcement and Hebbian learning. Behavioral reinforcement is most famously characterized through reinforcement learning techniques [39], but the core mechanism itself can be implemented using a host of other techniques [40]. For example, Hebbian learning is the dictum that “neurons that fire together, wire together” [41]. The co-occurence of particular neural units can produce spontaneous and adaptive behaviors depending on the context. Another example comes from the implementation of genetic algorithms, where fitness functions can serve to reinforce adaptive behaviors through hill-climbing on a fitness landscape [42]. More generally, our approaches involve a mechanism that allows for some form of adaptive feedback. Even in lower-capacity cognitive agents, a greater ability to model environmental conditions or interpret sensory-motor input may lend towards these agents developing into so-called good regulators [43]. This can be achieved through regulatory mechanisms for a single agent, or regulation of behaviors across multiple agents.

### Methods
This section presents the methods used to develop the software instantiations presented in the Results section. These include descriptions of software packages, and mathematical formalisms that describe each approach to our common problem.

#### BraGenBrain
The BraGenBrain approach utilizes a BV-genetic algorithm hybrid approach to produce adjacency matrices representing small connectomes. The use of operators such as crossover, mutation, and selection are used to introduce developmental plasticity, while the best performing developmental trajectories are discovered using natural selection. As the BV agents move around and interact in a sandbox simulation, agents develop both implicit (nervous system) as well as explicit (behavior) features. 

Environment and body. The BraGenBrain environment is a n-dimensional “box” of predefined size in pixels (which makes the suite screen size-independent) with so-called “world objects” across which the agents move. At the moment of writing, we have only conducted experiments in a two-dimensional space with one type of world objects defined as perfect circles of equal size. Yet extension of the program to additional dimensions is possible. An agent body incorporates many of the classic BV elements [5]. These include a body core, sensors that receive signals from world objects (the nature of these signals is defined later) as well as motors that move the whole body, that is body core, sensors and motors themselves based on the signal received by sensors (that is, sensors and motors are interconnected). Currently, we have conducted experiments with body of pre-defined “primitive Braitenberg” (rectangular) shape with sensors and motors circles small enough to mind them little in calculations, however the agent class (Vehicle) was supplied with a companion factory

__Table 1__. Constructing a vehicle in the BraGenBrain environment.
                                                                      |
class Vehicle {                                                       |
	//internal vehicle fields and methods                         |
	...                                                           |
	companion object Factory {                                    |
		...                                                   |
		fun simpleBVVehicle(...) : Vehicle {...}              |
		//other vehicle-producing functions                   |
		...                                                   |
	}                                                             |
}                                                                     |
----------------------------------------------------------------------|









Moreover, a depression function

$${\Delta}{W_ij} = {\dfrac{\phi} {50 {W^2_ij}{I_j} + 1}} \tag{1}$$

that naively imitates activity-dependent long-term depression is used to cancel the effect of repeatedly learning from one stimulus source and noisy data. Its effect was demonstrated through static testing where the BV does not move and stimuli are presented without priming, yet not demonstrated in the actual simulation. The associative memory is implemented as a bidirectional associative memory model in Layers.BAM class.

__Code.__ This project uses Cython and C. The most time consuming parts are either written in core.c or implemented by using OpenBLAS.. Static images such as those shown above are produced through Networkx and Matplotlib, while real-time animation is generated using PyQtGraph and PyQt5.


### Results

#### Computational model of Developmental Neuroscience
One goal of this project is to create a generalized computational model of neurodevelopment. This will allow us to investigate a large number of potential research questions. In general, we have found that there are three ways to approach an approximation of development and plasticity. Two of these approaches are a forward mapping, and the third is an inverse mapping. The first is to use a correlation (or covariance) matrix approach, where all neural units in the nervous system are compared with every other neural unit. This results in pairwise comparisons that can lead to connectome network maps [19]. The second approach is to add nodes and arcs sequentially to a simple set of I/O connections. In this case, we get a more explicit network topology, and can observe phenomena such as preferential attachment. A third approach is to prune connections from a fully-formed network engaged in hard-wired behaviors. Using this approach, one can come to understand exactly which connections and neurons are essential for the execution of a behavior.

#### Software Instantiations.
Currently, there are three software instantiations of the software: BraGenBrain, modeling neural plasticity using multisensory inputs, and BVs as Deep Learning. We will now discuss their details and limitations.  

__BraGenBrain: a genetic algorithmic approach.__ 

__Modeling Neural Plasticity using Multisensory Inputs.__ This instantiation is to create a robust and efficient simulation of Hebbian plasticity in learning and memory. The simulation utilized a Braitenberg Vehicle (BV) that possesses an olfactory system (smell), a gustatory system (taste), an associative memory, a motor unit, and a judgement unit. The BV is allowed to explore freely in an environment where sources of olfactory and gustatory stimuli are distributed.

During its exploration, the BV associates taste with smell when both taste and smell information are available. When there is no taste, it recalls the taste based on its associative memory and the smell received. Tastes are both sensed and recalled, which can produce preference that affects the BV's movement. When the BV becomes more and more mature via association, it can exhibit avoidance and preference behaviors, in a manner similar to small animals. An example of the simulation is shown in Figure 1.

<p align="center">
  <img width="600" height="300" src="https://github.com/Orthogonal-Research-Lab/GSoC-Braitenberg-Vehicles/blob/master/Manuscript/fig-4.png"><BR>
  Figure 1. An example of real-time animation. Experiments are conducted using an iPython Jupyter Notebook.
</p>
 
_Environment._ The environment is realistic that olfactory stimuli decay with distances exponentially from their sources, while gustatory stimuli are sensible only when the BV is within gustatory boundaries of those stimuli. The mapping between olfactory attributes and gustatory attributes should be defined before initializing Space.Space class and Simulation class. These outputs can be represented using an odor space and a taste space, respectively (Figure 2).

<p align="center">
  <img width="345" height="323" src="https://github.com/Orthogonal-Research-Lab/GSoC-Braitenberg-Vehicles/blob/master/Manuscript/fig-3-left.png"><img width="345" height="323" src="https://github.com/Orthogonal-Research-Lab/GSoC-Braitenberg-Vehicles/blob/master/Manuscript/fig-3-right.png"><BR>
  Figure 2. Odor space [20] of one olfactory attribute (left) and taste space of one gustatory attribute (right).
</p>

_Olfactory System._ The olfactory system, is implemented as a type of Li-Hopfield network [21], which is used as a standard model of olfactory bulb function (Figure 3). Li-Hopfield networks model the dynamics of two important cells in olfactory bulb: mitral cells and granule cells. Mitral cells take in relayed sensory information from receptor cells and glomeruli as input, and produce appropriate outputs to other parts of the brain [22]. Meanwhile, granule cells serve as inhibitors of mitral cell activity [23]. In a biological context, the ratio of granule cells than mitral cells is high. In this model, however, there are equal numbers of each.

<p align="center">
  <img width="450" height="450" src="https://github.com/Orthogonal-Research-Lab/GSoC-Braitenberg-Vehicles/blob/master/Manuscript/fig-1.png"><BR>
  Figure 3. Li-Hopfield-inspired model of the olfactory bulb. The grey dots are the mitral cells and the black are the granule cells. Red means excitation and blue means inhibition.
</p>

The Li-Hopfield network has been characterized as a group of coupled nonlinear oscillators [24]. In short, it is able to alter its oscillatory frequencies based on changes in olfactory attributes, so it is important to "filter" the noise and identify which stimulus source the BV is approaching. The signal powers of the output are then calculated, instead of modeling a complex afferent nerve in real nervous system. The olfactory system is implemented in Layers.LiHopfield class.

_Gustatory System._ In this model, the gustatory system is only a single layer of cells, for taste is simply an "impression" in this simulation. There is no noise involved in taste [25], or any other perturbation, so further processing of taste is redundant [26]. The gustatory system is implemented in Layers.Single class.

_Associative Memory._ The associative memory, implemented as a bidirectional associative memory (BAM), is how Hebbian learning is represented in this model (Figure 4). Rather than Hebbian rule that BAM often utilized, a Generalized Hebbian algorithm (GHA) is used, for it is demonstrably stable. The learning rate converges to zero with a constant rate to ensure the stability of GHA.

<p align="center">
  <img width="450 height="450" src="https://github.com/Orthogonal-Research-Lab/GSoC-Braitenberg-Vehicles/blob/master/Manuscript/fig-2.png"><BR>
  Figure 4. An example of a bidirectional associative memory (BAM) network. Nodes on the left are input cells, and the nodes on the right are output cells.
</p>

_Motor Unit._ The motor unit is radian-based. The BV moves along the heading direction whose value is in [-, ]. When the increase in preference passes a threshold, the BV moves forward with a little offset based on the increase; when the decrease in preference passes the threshold, the BV moves backward with a little offset based on the decrease. Otherwise, it moves towards a nearby source. The motor unit is implemented in Movement.RadMotor class. Because the learning rate of GHA has to decrease to ensure stability, the motor unit is equipped with memory to avoid repeated back-and-forth movement near the gustatory boundary of a “good” sample, which could easily lead to overfitting.

_Judgement Unit._ An array of preference function should be defined before initializing Simulation class. The preference, the output of the judgement unit, is the sum of the output of each preference functions applied to their corresponding gustatory attributes. The judgement unit is incorporated in Simulation class.


__BVs as a Deep Learning/Swarm model.__

### Discussion and Future Plans

Modeling Neural Plasticity using Multisensory Inputs. Future Plan. Put the trained BV in a new, testing environment, like conducting tests in animal models. Implement the progress saving functionality.



#### Use cases


Possible Implementation for modeling neural plasticity using multisensory inputs include more complex senses or more than two senses. More than 1 BV and BVs interactions. Another possible solution: apply genetic algorithm or other kinds to optimize the network structure.


### References
[1] Kaiser, M. (2017). Mechanisms of Connectome Development. _Trends in Cognitive Sciences_, 21(9), P703-P717. doi:10.1016/j.tics.2017.05.010.

[2]  Alicea, B. (2017). The Emergent Connectome in Caenorhabditis elegans Embryogenesis. _BioSystems_, 173, 247-255. 

[3] Larson, S.D., Gleeson, P., & Brown, A.E.X. (2018). Connectome to behaviour: modelling Caenorhabditis elegans at cellular resolution. _Philosophical Transactions of the Royal Society of London B_, 373(1758), 20170366. doi:10.1098/rstb.2017.0366.

[4] Fan, X. & Markram, H. (2019). A Brief History of Simulation Neuroscience. _Frontiers in Neuroinformatics_, doi:10.3389/fninf.2019.00032.

[5] Braitenberg, V. (1984). Vehicles: experiments in synthetic Psychology. MIT Press, Cambridge, MA.

[6] Seung, S. (2009). Connectome: How the Brain's Wiring Makes Us Who We Are. Houghton-Mifflin, Boston.

[7] Stiles, J. & Jernigan, T.L. (2010). The Basics of Brain Development. _Neuropsychological Review_, 20(4), 327–348. doi:10.1007/s11065-010-9148-4.

[8] Smith, J.L. & Schoenwolf, G.C. (1997). Neurulation: coming to closure. _Trends in Neurosciences_, 20(11), P510-P517. doi:10.1016/S0166-2236(97)01121-1.

[9] Eichler, K., Li, F., Litwin-Kumar, A., Park, Y., Andrade, I., Schneider-Mizell, C.M., Saumweber, T., Huser, A., Eschbach, C., Gerber, B., Fetter, R.D., Truman, J.W., Priebe, C.E., Abbott, L.F., Thum, A.S., Zlatic, M., & Cardona, A. (2017). The complete connectome of a learning and memory centre in an insect brain. _Nature_, 548(7666), 175-182. doi:10.1038/nature23455

[10] Craddock, R.C., Tungaraza, R.L., & Milham, M.P. (2015). Connectomics and new approaches for analyzing human brain functional connectivity. _Gigascience_, 4, 13.

[11] Towlson, E.K., Vertes, P.E., Ahnertm, S.E., Schafer, W.R., & Bullmore, E.T. (2013). The rich club of the C. elegans neuronal connectome. _Journal of Neuroscience_, 33(15), 6380-6387. doi:10.1523/JNEUROSCI.3784-12.2013.

[12] Sabrin, K.M & Dovrolis, C. (2017). The hourglass effect in hierarchical dependency networks. _Network Science_, 5(4), 490-528.

[13] Kriegeskorte, N. & Douglas, P.K. (2018). Cognitive computational neuroscience. _Nature Neuroscience_, 21, 1148–1160.

[14] Couzin, I. & Krause, J. (2003). Self-Organization and Collective Behavior in Vertebrates. _Advances in the Study of Behavior_, 32, 1-75 doi:10.1016/S0065-3454(03)01001-5.

[15] Neftci, E.O. & Averbeck, B.B. (2019). Reinforcement learning in artificial and biological systems. _Nature Machine Intelligence_, 1, 133–143. 

[16] Drugan, M.M. (2019). Reinforcement learning versus evolutionary computation: A survey on hybrid algorithms. _Swarm and Evolutionary Computation_, 44, 228-246.

[17] Munakata, Y. & Pfaffly, J. (2004). Hebbian learning and development. _Developmental Science_, 7(2), 141–148.

[18] Khaluf, Y., Ferrante, E., Simoens, P., & Huepe, C. (2017). Scale invariance in natural and artificial collective systems: a review. _Journal of the Royal Society Interface_, 14, 20170662. doi:10.1098/rsif.2017.0662.

[19] Henriksen, S., Pang, R., & Wronkiewicz, M. (2016). A simple generative model of the mouse mesoscale connectome. _eLife_, 5, e12366. doi:10.7554/eLife.12366.

[20] Soh, Z., Nishikawa, S., Kurita, Y., Takiguchi, N., & Tsuji, T. (2016). A Mathematical Model of the Olfactory Bulb for the Selective Adaptation Mechanism in the Rodent Olfactory System. _PLoS One_, 11(12), e0165230. doi: 10.1371/journal.

[21] Li, Z. & Hopfield, J.J. (1989). Modeling the Olfactory Bulb and its Neural Oscillatory Processings. _Biological Cybernetics_, 61, 379-392.

[22] Nagayama, S., Enerva, A., Fletcher, M. L., Masurkar, A.V., Igarashi, K.M., Mori, K., & Chen, W.R. (2010). Differential axonal projection of mitral and tufted cells in the mouse main olfactory system. _Frontiers in Neural Circuits_, 4(120). doi: 10.3389/fncir.2010.00120.

[23] Shepherd, G.M., Chen, Willhite, W.R. D., Migliore, M., & Greer C.A. (2007). The olfactory granule cell: From classical enigma to central role in olfactory processing. _Brain Research Reviews_, 55, 373-382.

[24] Sanger, T. D. (1989). Optimal Unsupervised Learning in a Single-Layer Linear Feedforward Neural Network. _Neural Networks_, 2, 459-473.

[25] Smith, D.V. & St John, S.J. (1999). Neural coding of gustatory information. _Current Opinion in Neurobiology_, 9, 427-435.

[26] Wu, A., Dvoryanchikov, G., Pereira, E., Chaudhari, N., & Roper, S. D. (2015). Breadth of tuning in taste afferent neurons varies with stimulus strength. _Nature Communications_, 6, 8171. doi: 10.1038/ncomms9171.


